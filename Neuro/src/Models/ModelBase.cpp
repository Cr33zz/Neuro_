#include <algorithm>
#include <iostream>
#include <numeric>
#include <cctype>
#include <iomanip>
#include <memory>
#include <H5Cpp.h>

#include "Models/ModelBase.h"
#include "Optimizers/OptimizerBase.h"
#include "Loss.h"
#include "Tools.h"
#include "Debug.h"
#include "ChartGenerator.h"
#include "Stopwatch.h"
#include "ComputationalGraph/Ops.h"
#include "ComputationalGraph/Placeholder.h"
#include "ComputationalGraph/Variable.h"
#include "ComputationalGraph/NameScope.h"
#include "ComputationalGraph/Trainer.h"
#include "ComputationalGraph/Predicter.h"

using namespace H5;

namespace Neuro
{
    //////////////////////////////////////////////////////////////////////////
    ModelBase::~ModelBase()
    {
        delete m_Optimizer;
    }

    //////////////////////////////////////////////////////////////////////////
    ModelBase::ModelBase(const string& constructorName, const string& name, int seed)
        : LayerBase(constructorName, Shape(), name)
    {
        NameScope scope(name);
        m_TrainingPlaceholder = new Placeholder(Shape(1), "training_phase");

        // output shape will be established when layers are added
        if (seed > 0)
        {
            m_Seed = seed;
            GlobalRngSeed(seed);
        }
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::OnClone(const LayerBase& source)
    {
        __super::OnClone(source);

        auto& sourceModel = static_cast<const ModelBase&>(source);
        m_Seed = sourceModel.m_Seed;
        m_Optimizer = sourceModel.m_Optimizer ? sourceModel.m_Optimizer->Clone() : nullptr;
    }
    
    //////////////////////////////////////////////////////////////////////////
    /*void ModelBase::Build(const vector<Shape>& inputShapes)
    {
        auto lastInputShapes = inputShapes;
        for (auto layer : m_Layers)
        {
            layer->Build(lastInputShapes);
            lastInputShapes = layer->OutputShapes();
        }
    }*/

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::InitGraph(const vector<TensorLike*>& inputs, const vector<TensorLike*>& outputs)
    {
        m_Inputs = inputs;
        m_Outputs = outputs;

        // check if each input is provided only once
        unordered_set<TensorLike*> redundancyCheck;
        redundancyCheck.insert(inputs.begin(), inputs.end());

        NEURO_ASSERT(redundancyCheck.size() == inputs.size(), "The list of inputs passed to the model is redundant. All inputs should only appear once.");

        // do some validation first
        for (size_t i = 0; i < inputs.size(); ++i)
        {
            auto x = inputs[i];
            
            NEURO_ASSERT(x->m_Metadata, "Input tensors to a " << ClassName() << " " << "must come from 'Input'. Received: " << x->Name() << " (missing previous layer metadata).");

            auto layer = x->m_Metadata->layer;

            // check if origin points to input layer
            NEURO_ASSERT(layer->m_InboundNodes.size() == 1 && (layer->m_InboundNodes.empty() || layer->m_InboundNodes[0]->inbound_layers.size() == 0),
                ClassName() + " inputs must come from 'Input' (thus holding past layer metadata), they cannot be the output of a previous non-Input layer.\nHere, a tensor specified as input to your model was not an Input tensor, it was generated by layer " <<
                layer->Name() << ".\nThe tensor that caused the issue was: " << x->Name());
        }

        for (size_t i = 0; i < outputs.size(); ++i)
            NEURO_ASSERT(outputs[i]->m_Metadata, "Output tensors to a " << ClassName() << " must be the output of a 'LayerBase' (thus holding past layer metadata). Found: " << outputs[i]->Name());

        m_Built = true; // no variables are stored in model itself
        m_GraphNetwork = true;

        m_InputLayers.clear();
        m_InputCoords.clear();
        m_OutputLayers.clear();
        m_OutputCoords.clear();

        for (size_t i = 0; i < outputs.size(); ++i)
        {
            auto x = outputs[i];

            m_OutputLayers.push_back(x->m_Metadata->layer);
            m_OutputCoords.push_back(x->m_Metadata);
        }

        for (size_t i = 0; i < inputs.size(); ++i)
        {
            auto x = inputs[i];

            NEURO_ASSERT(x->m_Metadata->node_index == 0, "");
            NEURO_ASSERT(x->m_Metadata->tensor_index == 0, "");
            m_InputLayers.push_back(x->m_Metadata->layer);
            m_InputCoords.push_back(x->m_Metadata);
        }

        m_Layers.clear();
        m_LayerNodes.clear();

        //MapGraphNetwork(m_Inputs, m_Outputs);
        unordered_set<LayerBase*> visited;
        for (auto layer : m_OutputLayers)
            ProcessLayer(layer, visited);

        // we have to add new inbound node because sequential model can be shared in its current state before adding more layers
        new node(this, {}, {}, {}, m_Inputs, m_Outputs, CollectShapes(m_Inputs), CollectShapes(m_Outputs));
    }

    //////////////////////////////////////////////////////////////////////////
    vector<TensorLike*> ModelBase::InternalCall(const vector<TensorLike*>& inputs, TensorLike* training)
    {
        NEURO_ASSERT(inputs.size() == m_Inputs.size(), "Number of inputs doesn't match. Expected " << m_Inputs.size() << " received " << inputs.size());

        map<LayerBase*, vector<TensorLike*>> outputsPerLayer;

        // all input layers are single tensor placeholders
        for (size_t i = 0; i < m_InputLayers.size(); ++i)
        {
            auto layer = m_InputLayers[i];
            outputsPerLayer[layer] = layer->Call(inputs[i], training);
        }

        for (size_t i = 0; i < m_Layers.size(); ++i)
        {
            auto layer = m_Layers[i];

            if (find(m_InputLayers.begin(), m_InputLayers.end(), layer) != m_InputLayers.end())
                continue; // we already called input layers

            auto inboundNode = m_LayerNodes[i];

            // gather inputs from inbound layers and call
            vector<TensorLike*> layerInputs;
            for (auto inboundLayer : inboundNode->inbound_layers)
            {
                auto& inboundLayerOutputs = outputsPerLayer[inboundLayer];
                layerInputs.insert(layerInputs.end(), inboundLayerOutputs.begin(), inboundLayerOutputs.end());
            }

            outputsPerLayer[layer] = layer->Call(layerInputs, training);
        }

        // gather outputs from output layers
        vector<TensorLike*> outputs;
        for (auto layer : m_OutputLayers)
        {
            NEURO_ASSERT(outputsPerLayer.find(layer) != outputsPerLayer.end(), "Output layer '" << layer->Name() << "' was not called.");
            auto& layerOutputs = outputsPerLayer[layer];
            outputs.insert(outputs.end(), layerOutputs.begin(), layerOutputs.end());
        }

        // destroy metadata in output tensors because this model will 'claim' their 'ownership'
        for_each(outputs.begin(), outputs.end(), [](TensorLike* output) { delete output->m_Metadata; output->m_Metadata = nullptr; });
        
        return outputs;
    }

    //////////////////////////////////////////////////////////////////////////
    vector<TensorLike*> ModelBase::GetSourceInputs(TensorLike* tensor, LayerBase* layer, int nodeIndex)
    {
        if (!tensor->m_Metadata)
            return { tensor };

        if (!layer || nodeIndex >= 0)
        {
            layer = tensor->m_Metadata->layer;
            nodeIndex = (int)tensor->m_Metadata->node_index;
        }

        if (layer->m_InboundNodes.empty())
            return { tensor };
        else
        {
            auto node = layer->m_InboundNodes[nodeIndex];

            if (node->inbound_layers.empty())
            {
                // Reached an Input layer, stop recursion.
                return node->input_tensors;
            }
            else
            {
                vector<TensorLike*> source_tensors;
                unordered_set<TensorLike*> visitedTensors;
                for (size_t i = 0; i < node->inbound_layers.size(); ++i)
                {
                    auto x = node->input_tensors[i];
                    auto layer = node->inbound_layers[i];
                    auto nodeIndex = node->node_indices[i];
                    auto previousSources = GetSourceInputs(x, layer, nodeIndex);

                    // Avoid input redundancy.
                    for (auto x : previousSources)
                    {
                        if (visitedTensors.find(x) == visitedTensors.end())
                        {
                            source_tensors.push_back(x);
                            visitedTensors.insert(x);
                        }
                    }
                }
                return source_tensors;
            }
        }
    }

    //////////////////////////////////////////////////////////////////////////
    tensor_ptr_vec_t ModelBase::Predict(const const_tensor_ptr_vec_t& inputs)
    {
        if (!m_Predicter)
        {
            vector<Placeholder*> inputs;
            for_each(m_Inputs.begin(), m_Inputs.end(), [&](TensorLike* input) { inputs.push_back(static_cast<Placeholder*>(input)); });
            m_Predicter = new Predicter(inputs, m_Outputs, m_TrainingPlaceholder);
        }

        return m_Predicter->Predict(inputs);
    }

    //////////////////////////////////////////////////////////////////////////
    tensor_ptr_vec_t ModelBase::Predict(const Tensor& input)
    {
        const_tensor_ptr_vec_t inputs{ &input };
        return Predict(inputs);
    }

    //////////////////////////////////////////////////////////////////////////
    tensor_ptr_vec_t ModelBase::Eval(const vector<TensorLike*>& fetches, const map<Placeholder*, const Tensor*>& feeds)
    {
        size_t fetchesHash = 0;
        std::hash<TensorLike*> hasher;
        for (size_t i = 0; i < fetches.size(); ++i)
            fetchesHash = fetchesHash * 31 + hasher(fetches[i]);

        auto predicter = m_EvalPredicters[fetchesHash];

        if (!predicter)
        {
            vector<TensorLike*> outputs;
            for_each(fetches.begin(), fetches.end(), [&](TensorLike* output) { outputs.push_back(output); });
            predicter = m_EvalPredicters[fetchesHash] = new Predicter({}, outputs, m_TrainingPlaceholder);
        }

        return predicter->Eval(feeds);
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::Optimize(OptimizerBase* optimizer, LossBase* loss)
    {
        map<string, LossBase*> lossDict;
        for (auto outLayer : m_OutputLayers)
            lossDict[outLayer->Name()] = loss;

        Optimize(optimizer, lossDict);
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::Optimize(OptimizerBase* optimizer, map<string, LossBase*> lossDict)
    {
        NameScope scope(Name());

        m_Optimizer = optimizer;

        NEURO_ASSERT(lossDict.size() == m_OutputLayers.size(), "Each output layer must have a corresponding loss function provided.");

        //m_AccuracyFuncs.resize(outputsShapes.size());

        vector<Placeholder*> targets;
        vector<TensorLike*> fetches;

        vector<TensorLike*> losses;
        TensorLike* totalLoss = nullptr;

        {
            NameScope scope("loss");

            for (size_t i = 0; i < m_Outputs.size(); ++i)
            {
                auto output = m_Outputs[i];
                auto layer = output->m_Metadata->layer;

                //m_AccuracyFuncs[i] = outputsShapes[i].Length == 1 ? AccBinaryClassificationEquality : AccCategoricalClassificationEquality;

                targets.push_back(new Placeholder(Shape(output->GetShape()), "target" + to_string(i)));
                auto loss = lossDict[layer->Name()]->Build(targets.back(), output);

                losses.push_back(loss);
                fetches.push_back(loss);

                if (!totalLoss)
                    totalLoss = mean(loss, GlobalAxis, "mean_loss" + to_string(i));
                else
                    totalLoss = merge_sum({ totalLoss, mean(loss) }, "total_loss");
            }
        }

        fetches.push_back(totalLoss);
        m_Metrics["loss"] = make_pair(totalLoss, fetches.size() - 1);
        // any additional metrics should go in here

        vector<Variable*> params;
        Parameters(params);

        fetches.push_back(optimizer->Minimize(losses, params));

        vector<Placeholder*> inputs;
        for_each(m_Inputs.begin(), m_Inputs.end(), [&](TensorLike* input) { inputs.push_back(static_cast<Placeholder*>(input)); });

        m_Trainer = new Trainer(inputs, targets, fetches, m_TrainingPlaceholder);
    }

    //////////////////////////////////////////////////////////////////////////
    string ModelBase::Summary() const
    {
        stringstream ss;
        ss << "_________________________________________________________________\n";
        ss << "Layer                        Output Shape              Param #   \n";
        ss << "=================================================================\n";

        for (auto layer : Layers())
        {
            ss << left << setw(29) << (layer->Name() + "(" + layer->ClassName() + ")").substr(0, 28);
            ss << setw(26) << layer->OutputShapes()[0].ToString();
            ss << setw(13) << layer->ParamsNum() << "\n";
            auto& inboundLayers = layer->m_InboundNodes.back()->inbound_layers;
            if (inboundLayers.size() > 1)
            {
                for (int i = 0; i < (int)inboundLayers.size(); ++i)
                    ss << inboundLayers[i]->Name() << "\n";
            }
            ss << "_________________________________________________________________\n";
        }

        ss << "Total params: " << ParamsNum() << endl;
        ss << "Total trainable params: " << TrainableParamsNum() << endl;
        ss << "Total non-trainable params: " << NonTrainableParamsNum() << endl;
        return ss.str();
    }

    //////////////////////////////////////////////////////////////////////////
    string ModelBase::TrainSummary() const
    {
        stringstream ss;
        ss.precision(3);
        int totalParams = 0;
        ss << "_____________________________________________________________________________\n";
        ss << "Layer                        Fwd[s]      Back[s]     ActFwd[s]   ActBack[s]  \n";
        ss << "=============================================================================\n";

        for (auto layer : Layers())
        {
            /*ss << left << setw(29) << (layer->Name() + "(" + layer->ClassName() + ")").substr(0, 28);
            ss << setw(12) << layer->FeedForwardTime() * 0.001f;
            ss << setw(12) << layer->BackPropTime() * 0.001f;
            ss << setw(12) << layer->ActivationTime() * 0.001f;
            ss << setw(12) << layer->ActivationBackPropTime() * 0.001f << "\n";*/
            ss << "_____________________________________________________________________________\n";
        }

        return ss.str();
    }

    //////////////////////////////////////////////////////////////////////////
	LayerBase* ModelBase::Layer(const string& name)
	{
		for (auto layer : Layers())
		{
			if (layer->Name() == name)
				return layer;
		}
		return nullptr;
	}

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::SaveWeights(const string& filename) const
    {
        //https://github.com/keras-team/keras/blob/5be4ed3d9e7548dfa9d51d1d045a3f951d11c2b1/keras/engine/saving.py#L733
        H5File file = H5File(filename, H5F_ACC_TRUNC);
        
        Tensor tranposedParam;
        vector<SerializedParameter> params;
        vector<string> paramNames;

        {
            Attribute att(file.createAttribute("nb_layers", PredType::NATIVE_INT64, DataSpace(H5S_SCALAR)));
            int64_t layersNum = (int64_t)Layers().size();
            att.write(PredType::NATIVE_INT64, &layersNum);
        }

        int layerIdx = 0;
        for (auto layer : Layers())
        {
            Group g(file.createGroup("layer" + to_string(layerIdx++)));

            params.clear();
            paramNames.clear();
            layer->SerializedParameters(params);

            Attribute att(g.createAttribute("nb_params", PredType::NATIVE_INT64, DataSpace(H5S_SCALAR)));
            int64_t paramsNum = (int64_t)params.size();
            att.write(PredType::NATIVE_INT64, &paramsNum);
            
            for (auto i = 0; i < params.size(); ++i)
            {
                auto w = params[i].param->OutputGradPtr();
                auto& wShape = w->GetShape();
                
                vector<hsize_t> dims;
                for (uint32_t i = 0; i < wShape.NDim; ++i)
                    dims.push_back(wShape.Dimensions[i]);

                DataSet dataset(g.createDataSet("param_" + to_string(i), PredType::NATIVE_FLOAT, DataSpace(wShape.NDim, &dims[0])));
                dataset.write(&w->GetValues()[0], PredType::NATIVE_FLOAT);
            }
        }

        /*ofstream stream(filename, ios::out | ios::binary);
        vector<ParametersAndGradients> params;
        const_cast<ModelBase*>(this)->ParametersAndGradients(params, false);
        for (auto i = 0; i < params.size(); ++i)
            params[i].Parameters->SaveBin(stream);
        stream.close();*/
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::LoadWeights(const string& filename, bool ignoreInputLayer)
    {
        if (!m_Built)
            Build();

        H5File file = H5File(filename, H5F_ACC_RDONLY);

        bool is_keras = file.attrExists("layer_names");

        Shape tranposedParamShape;
        Tensor kerasParam;
        vector<SerializedParameter> params;
        static char buffer[1024];

        auto layers = Layers();
        if (ignoreInputLayer)
            layers.erase(layers.begin()); // remove input layer from the list

        hsize_t layerGroupsNum;
        H5Gget_num_objs(file.getId(), &layerGroupsNum);

        NEURO_ASSERT((size_t)layerGroupsNum == layers.size(), "Number of saved layers doesn't match number of layers in the model. Found " << layerGroupsNum << " expected " << layers.size() << ".");

        vector<hsize_t> layersOrder(layers.size());
        iota(layersOrder.begin(), layersOrder.end(), 0); // creation order by default

        // Keras specifies order of layers by attribute containing array of layer names
        if (is_keras)
        {
            map<string, hsize_t> layerNameToIdx;
            for (hsize_t i = 0; i < layerGroupsNum; ++i)
                layerNameToIdx[file.getObjnameByIdx(i)] = i;

            Attribute att(file.openAttribute("layer_names"));
            hsize_t layersNamesNum = 0;
            att.getSpace().getSimpleExtentDims(&layersNamesNum);
            NEURO_ASSERT(layersNamesNum == layerGroupsNum, "Number of layer names doesn't match number of layers' groups. Found " << layersNamesNum << " expected " << layerGroupsNum << ".");
            hsize_t strLen = att.getDataType().getSize();
            att.read(att.getDataType(), buffer);

            for (hsize_t i = 0; i < layerGroupsNum; ++i)
            {
                string layerName(buffer + i * strLen, min(strlen(buffer + i * strLen), strLen));
                // we need to get rid of group/layer name from weight name
                layerName = layerName.substr(layerName.find_last_of('/') + 1);
                layersOrder[i] = layerNameToIdx[layerName];
            }
        }

        for (size_t l = 0; l < layers.size(); ++l)
        {
            auto layer = layers[l];

            Group g(file.openGroup(file.getObjnameByIdx(layersOrder[l])));

            params.clear();
            layer->SerializedParameters(params);

            vector<DataSet> weightsDatasets;

            // Keras specifies order of tensors by attribute containing array of tensor names
            if (is_keras)
            {
                Attribute att(g.openAttribute("weight_names"));
                hsize_t weightsNamesNum = 0;
                att.getSpace().getSimpleExtentDims(&weightsNamesNum);
                NEURO_ASSERT(weightsNamesNum == params.size(), "Number of saved parameters doesn't match number of parameters in layer '" << layer->Name() << "'. Found " << weightsNamesNum << " expected " << params.size() << ".");
                hsize_t strLen = att.getDataType().getSize();
                att.read(att.getDataType(), buffer);

                for (hsize_t i = 0; i < weightsNamesNum; ++i)
                {
                    string weightName(buffer + i * strLen, min(strlen(buffer + i * strLen), strLen));
                    weightsDatasets.push_back(g.openDataSet(weightName));
                }
            }
            else
            {
                for (hsize_t i = 0; i < params.size(); ++i)
                    weightsDatasets.push_back(g.openDataSet(g.getObjnameByIdx(i)));
            }

            for (hsize_t i = 0; i < params.size(); ++i)
            {
                auto& dataset = weightsDatasets[i];
                auto w = params[i].param->OutputPtr();

                hsize_t weightNDims = dataset.getSpace().getSimpleExtentNdims();
                hsize_t weightDims[5];
                dataset.getSpace().getSimpleExtentDims(nullptr, weightDims);

                NEURO_ASSERT(w->GetShape().Length == dataset.getSpace().getSimpleExtentNpoints(), "Number of values in parameter '" << w->Name() << "' doesn't match saved parameter. Found " << dataset.getSpace().getSimpleExtentNpoints() << " expected " << w->GetShape().Length  << ".");

                if (is_keras && !params[i].transAxesKeras.empty())
                {
                    vector<int> dims(weightNDims);
                    for (size_t n = 0; n < dims.size(); ++n)
                        dims[n] = (int)weightDims[n];
                    kerasParam.Resize(Shape::FromKeras(&dims[0], (int)weightNDims));
                    kerasParam.Name(w->Name());
                    w = &kerasParam;
                }

                auto wShape = w->GetShape();

                NEURO_ASSERT(wShape.NDim == dataset.getSpace().getSimpleExtentNdims(), "Number of dimensions of parameter '" << w->Name() << "' doesn't match saved parameter. Found " << dataset.getSpace().getSimpleExtentNdims() << " expected " << wShape.NDim << ".");
                for (int i = wShape.NDim - 1, n = 0; i >= 0; --i, ++n)
                    NEURO_ASSERT(weightDims[n] == wShape.Dimensions[i], "Dimension " << i << " of parameter '" << w->Name() << "' doesn't match corresponding dimension of saved parameter. Found " << weightDims[i] << " expected " << wShape.Dimensions[i] << ".");

                dataset.read(&w->GetValues()[0], PredType::NATIVE_FLOAT);

                if (is_keras && !params[i].transAxesKeras.empty())
                {
                    params[i].param->Output() = w->Transposed(params[i].transAxesKeras);
                    params[i].param->Output().Name(kerasParam.Name());
                }

                params[i].param->ForceInitialized();
            }
        }

        /*ifstream stream(filename, ios::in | ios::binary);
        vector<ParametersAndGradients> params;
        ParametersAndGradients(params, false);
        for (auto i = 0; i < params.size(); ++i)
            params[i].Parameters->LoadBin(stream);
        stream.close();*/
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::Parameters(vector<Variable*>& params, bool onlyTrainable) const
    {
        if (onlyTrainable && !m_Trainable)
            return;

        for (auto layer : Layers())
            layer->Parameters(params, onlyTrainable);
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::SetTrainable(bool trainable)
    {
        m_Trainable = trainable;

        for (auto layer : Layers())
            layer->SetTrainable(trainable);
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::Fit(const Tensor& input, const Tensor& output, int batchSize, uint32_t epochs, const Tensor* validInput, const Tensor* validOutput, uint32_t verbose, int trackFlags, bool shuffle)
    {
        const_tensor_ptr_vec_t validInputs = { validInput }, validOutputs = { validOutput };
        Fit({ &input }, { &output }, batchSize, epochs, validInput ? &validInputs : nullptr, validOutput ? &validOutputs : nullptr, verbose, trackFlags, shuffle);
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::Fit(const const_tensor_ptr_vec_t& inputs, const const_tensor_ptr_vec_t& outputs, int batchSize, uint32_t epochs, const const_tensor_ptr_vec_t* validInputs, const const_tensor_ptr_vec_t* validOutputs, uint32_t verbose, int trackFlags, bool shuffle)
    {
        cout << unitbuf; // disable buffering so progress 'animations' can work

        assert((validInputs && validOutputs) || (!validInputs && !validOutputs));
        //assert(inputs.size() == GetInputLayersCount());
        assert(outputs.size() == m_OutputLayers.size());

        uint32_t trainSamplesCount = inputs[0]->Batch();
        uint32_t validationSamplesCount = validInputs ? (*validInputs)[0]->Batch() : 0;

        for (auto inputTensor : inputs)
            assert(inputTensor->Batch() == trainSamplesCount && "Number of batches across all inputs must match.");
        for (auto outputTensor : outputs)
            assert(outputTensor->Batch() == trainSamplesCount && "Number of batches across all outputs must match number or batches in inputs.");

        uint32_t trainBatchSize = batchSize < 0 ? trainSamplesCount : batchSize;
        uint32_t validationBatchSize = batchSize < 0 ? validationSamplesCount : min(validationSamplesCount, (uint32_t)batchSize);

        string outFilename = FilePrefix() + "_training_data_" + m_Optimizer->ClassName() + "_b" + to_string(trainBatchSize) + (m_Seed > 0 ? "(seed" + to_string(m_Seed) + ")" : "");

        if (verbose > 0)
            m_LogFile = new ofstream(outFilename + ".log");

        ChartGenerator* chartGen = nullptr;
        if (trackFlags != Nothing)
            chartGen = new ChartGenerator(outFilename, Name()/* + "\nloss=" + [{string.Join(", ", Losses.Select(x => x.GetType().Name))}] optimizer={Optimizer} batch_size={trainBatchSize}\nseed={(Seed > 0 ? Seed.ToString() : "None")}"*/, "Epoch");

        if (trackFlags & TrainError)
            chartGen->AddSeries((int)TrainError, "Error on train data\n(left Y axis)", 2/*Color.DarkRed*/);
        if (trackFlags & TestError)
            chartGen->AddSeries((int)TestError, "Error on test data\n(left Y axis)", 2/*Color.IndianRed*/);
        if (trackFlags & TrainAccuracy)
            chartGen->AddSeries((int)TrainAccuracy, "Accuracy on train data\n(right Y axis)", 2/*Color.DarkBlue*/, true);
        if (trackFlags & TestAccuracy)
            chartGen->AddSeries((int)TestAccuracy, "Accuracy on test\n(right Y axis)", 2/*Color.CornflowerBlue*/, true);

        vector<uint32_t> indices(trainSamplesCount);
        iota(indices.begin(), indices.end(), 0);

        uint32_t trainBatchesNum = (uint32_t)ceil(trainSamplesCount / (float)trainBatchSize);
        uint32_t validationBatchesNum = validationBatchSize > 0 ? (uint32_t)ceil(validationSamplesCount / (float)validationBatchSize) : 0;
        vector<vector<uint32_t>> trainBatchesIndices(trainBatchesNum);

        vector<const_tensor_ptr_vec_t> validInputsBatches, validOutputsBatches;
        if (validInputs)
        {
            uint32_t i = 0;
            vector<uint32_t> validationBatchIndices;
            for (uint32_t b = 0; b < validationBatchesNum; ++b)
            {
                uint32_t samplesStartIndex = b * validationBatchSize;
                uint32_t samplesEndIndex = min((b + 1) * validationBatchSize, validationSamplesCount);
                validationBatchIndices.resize(samplesEndIndex - samplesStartIndex);
                iota(validationBatchIndices.begin(), validationBatchIndices.end(), i);
                i += (uint32_t)validationBatchIndices.size();
                validInputsBatches.push_back(GenerateBatch(*validInputs, validationBatchIndices));
                validOutputsBatches.push_back(GenerateBatch(*validOutputs, validationBatchIndices));
            }
        }

        for (uint32_t e = 1; e <= epochs; ++e)
        {
            if (verbose > 0)
                LogLine("Epoch " + to_string(e) + "/" + to_string(epochs));

            // no point generating batches when we have single batch
            if (trainSamplesCount > 1 && trainBatchSize < trainSamplesCount)
            {
                if (shuffle)
                    random_shuffle(indices.begin(), indices.end(), [&](size_t max) { return GlobalRng().Next((int)max); });

                for (uint32_t b = 0; b < trainBatchesNum; ++b)
                {
                    uint32_t samplesStartIndex = b * trainBatchSize;
                    uint32_t samplesEndIndex = min((b + 1) * trainBatchSize, trainSamplesCount);
                    trainBatchesIndices[b].resize(samplesEndIndex - samplesStartIndex);
                    copy(indices.begin() + samplesStartIndex, indices.begin() + samplesEndIndex, trainBatchesIndices[b].begin());
                }
            }

            float trainTotalLoss = 0;
            float trainTotalAcc = 0;

            unique_ptr<Tqdm> progress(verbose == 2 ? new Tqdm(trainSamplesCount) : nullptr);
            for (uint32_t b = 0; b < trainBatchesNum; ++b)
            {
                uint32_t samplesInBatch = inputs[0]->Batch();

                float loss, acc = 0;
                if (trainSamplesCount > 1 && trainBatchSize < trainSamplesCount)
                {
                    auto inputsBatch = GenerateBatch(inputs, trainBatchesIndices[b]);
                    auto outputsBatch = GenerateBatch(outputs, trainBatchesIndices[b]);

                    samplesInBatch = inputsBatch[0]->Batch();

                    TrainStep(inputsBatch, outputsBatch, &loss, (trackFlags & TrainAccuracy) ? &acc: nullptr);

                    DeleteContainer(inputsBatch);
                    DeleteContainer(outputsBatch);
                }
                else
                    TrainStep(inputs, outputs, &loss, &acc);

                trainTotalLoss += loss;
                trainTotalAcc += acc;

                if (progress)
                    progress->NextStep(samplesInBatch);
            }

            if (progress)
                LogLine(progress->Str(), false);

            float trainLoss = trainTotalLoss / trainBatchesNum;
            float trainAcc = (float)trainTotalAcc / trainBatchesNum;
            m_LastTrainError = trainLoss;

            if (chartGen)
            {
                chartGen->AddData((float)e, trainLoss, (int)TrainError);
                chartGen->AddData((float)e, trainAcc, (int)TrainAccuracy);
            }

            stringstream summary;
            summary.precision(4);

            if (verbose > 0)
            {
                if (trackFlags & TrainError)
                    summary << " - loss: " << trainLoss;
                if (trackFlags & TrainAccuracy)
                    summary << " - acc: " << trainAcc;
            }

            if (validInputs && validOutputs)
            {
                float validationTotalLoss = 0;
                float validationHits = 0;

                //auto& modelOutputs = Outputs();

                for (uint32_t b = 0; b < validationBatchesNum; ++b)
                {
                    /*FeedForward(validInputsBatches[b], false);

                    vector<Tensor> out;
                    for (size_t i = 0; i < modelOutputs.size(); ++i)
                    {
                        out.push_back(Tensor(modelOutputs[i]->GetShape()));
                        m_LossFuncs[i]->Compute(*validOutputsBatches[b][i], *modelOutputs[i], out[i]);

                        validationTotalLoss += out[i].Sum(GlobalAxis)(0) / modelOutputs[i]->BatchLength();
                        if (trackFlags & TestAccuracy)
                            validationHits += m_AccuracyFuncs[i](*validOutputsBatches[b][i], *modelOutputs[i]);
                    }

                    if (verbose == 2)
                    {
                        int processedValidationSamplesNum = min((b + 1) * validationBatchSize, validationSamplesCount);
                        string progressStr = " - validating: " + to_string((int)round(processedValidationSamplesNum / (float)validationSamplesCount * 100.f)) + "%";
                        cout << progressStr;
                        for (uint32_t i = 0; i < progressStr.length(); ++i)
                            cout << '\b';
                    }*/
                }

                float validationLoss = validationTotalLoss / validationSamplesCount / outputs.size();
                float validationAcc = (float)validationHits / validationSamplesCount / outputs.size();

                if (verbose > 0)
                {
                    if (trackFlags & TestError)
                        summary << " - val_loss: " << validationLoss;
                    if (trackFlags & TestAccuracy)
                        summary << " - val_acc: " << validationAcc;
                }

                /*chartGen?.AddData(e, testTotalError / validationSamples, (int)Track::TestError);
                chartGen?.AddData(e, (float)testHits / validationSamples / outputLayersCount, (int)Track::TestAccuracy);*/
            }

            if (verbose > 0)
                LogLine(summary.str());

            /*if ((ChartSaveInterval > 0 && (e % ChartSaveInterval == 0)) || e == epochs)
                chartGen ? .Save();*/
        }

        for (size_t b = 0; b < validInputsBatches.size(); ++b)
        {
            DeleteContainer(validInputsBatches[b]);
            DeleteContainer(validOutputsBatches[b]);
        }

        if (m_LogFile && m_LogFile->is_open())
        {
            m_LogFile->close();
            delete m_LogFile;
            m_LogFile = nullptr;
        }
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::MapGraphNetwork(const vector<TensorLike*>& inputs, const vector<TensorLike*>& outputs)
    {
        auto makeNodeKey = [](const string& layerName, int nodeIndex)
        {
            return layerName + "_ib-" + to_string(nodeIndex);
        };

        unordered_set<string> networkNodes; // ids of all nodes relevant to the Network
        map<node*, int> nodes_depths; // dict{ node: depth value }
        map<LayerBase*, int> layers_depths;  // dict{ layer: depth value }
        map<LayerBase*, int> layer_indices;  // dict{ layer: index in traversal }
        vector<node*> nodes_in_decreasing_depth;

        function<void(TensorLike*,unordered_set<node*>&,unordered_set<node*>&,LayerBase*,int,int)> buildMap;
        buildMap = [&](TensorLike* tensor, unordered_set<node*>& finishedNodes, unordered_set<node*>& nodesInProgress, LayerBase* layer, int nodeIndex, int tensorIndex)
        {
            auto node = layer->m_InboundNodes[nodeIndex].get();

            // Prevent cycles.
            if (find(nodesInProgress.begin(), nodesInProgress.end(), node) != nodesInProgress.end())
                NEURO_ASSERT(false, "The tensor " << tensor->Name() << " at layer '" << layer->Name() << "' is part of a cycle.");

            // Don't repeat work for shared subgraphs
            if (finishedNodes.find(node) != finishedNodes.end())
                return;

            //auto node_key = _make_node_key(layer.name, node_index)
            // Update network_nodes.
            //network_nodes.add(node_key)

            // Store the traversal order for layer sorting.
            if (layer_indices.find(layer) == layer_indices.end())
                layer_indices[layer] = (int)layer_indices.size();

            nodesInProgress.insert(node);

            // Propagate to all previous tensors connected to this node.
            for (size_t i = 0; i < node->inbound_layers.size(); ++i)
            {
                auto x = node->input_tensors[i];
                auto layer = node->inbound_layers[i];
                auto node_index = node->node_indices[i];
                auto tensor_index = node->tensor_indices[i];
                buildMap(x, finishedNodes, nodesInProgress, layer, node_index, tensor_index);
            }
                
            finishedNodes.insert(node);
            nodesInProgress.erase(node);
            nodes_in_decreasing_depth.push_back(node);
        };

        unordered_set<node*> finished_nodes;
        unordered_set<node*> nodes_in_progress;
        
        for (auto x : outputs)
            buildMap(x, finished_nodes, nodes_in_progress, x->m_Metadata->layer, (int)x->m_Metadata->node_index, (int)x->m_Metadata->tensor_index);

        for (auto nodeIt = nodes_in_decreasing_depth.rbegin(); nodeIt != nodes_in_decreasing_depth.rend(); ++nodeIt)
        {
            auto node = *nodeIt;

            // If the depth is not set, the node has no outbound nodes(depth 0).
            int depth = 0;
            if (nodes_depths.find(node) == nodes_depths.end())
                nodes_depths[node] = 0;

            // Update the depth of the corresponding layer
            int previous_depth = layers_depths[node->outbound_layer];
            // If we've seen this layer before at a higher depth,
            // we should use that depth instead of the node depth.
            // This is necessary for shared layers that have inputs at different
            // depth levels in the graph.
            depth = max(depth, previous_depth);
            layers_depths[node->outbound_layer] = depth;
            nodes_depths[node] = depth;

            // Update the depth of inbound nodes.
            // The "depth" of a node is the max of the depths
            // of all layers it is connected to.
            for (size_t i = 0; i < node->inbound_layers.size(); ++i)
            {
                auto inbound_layer = node->inbound_layers[i];
                auto node_index = node->node_indices[i];
                auto inbound_node = inbound_layer->m_InboundNodes[node_index];
                int previous_depth = nodes_depths[inbound_node.get()];
                nodes_depths[inbound_node.get()] = max(depth + 1, previous_depth);
            }
        }
        
        // Build a dict{ depth: list of nodes with this depth }
        map<int, vector<node*>> nodes_by_depth;
        for (auto entry : nodes_depths)
        {
            auto node = entry.first;
            auto depth = entry.second;

            nodes_by_depth[depth].push_back(node);
        }

        // Build a dict{ depth: list of layers with this depth }
        map<int, vector<LayerBase*>> layers_by_depth;
        for (auto entry : layers_depths)
        {
            auto layer = entry.first;
            auto depth = entry.second;

            layers_by_depth[depth].push_back(layer);
        }

        // Get sorted list of layer depths.
        vector<int> depth_keys;
        for_each(layers_by_depth.begin(), layers_by_depth.end(), [&](const pair<int, vector<LayerBase*>>& entry) { depth_keys.push_back(entry.first); });
        reverse(depth_keys.begin(), depth_keys.end());

        // Set self.layers and self._layers_by_depth.
        /*    layers = []
        for (auto depth : depth_keys)
        {
            layers_for_depth = layers_by_depth[depth]
            // Network.layers needs to have a deterministic order :
            // here we order them by traversal order.
            layers_for_depth.sort(key = lambda x : layer_indices[x])
            layers.extend(layers_for_depth)
        }

            // Get sorted list of node depths.
            depth_keys = list(nodes_by_depth.keys())
            depth_keys.sort(reverse = True)
        */
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::ProcessLayer(LayerBase* layer, unordered_set<LayerBase*>& visited)
    {
        if (visited.find(layer) != visited.end())
            return;

        visited.insert(layer);

        auto& inputTensors = layer->InputsAt(-1); // get last inputs

        for (size_t i = 0; i < inputTensors.size(); ++i)
        {
            assert(inputTensors[i]->m_Metadata);
            ProcessLayer(inputTensors[i]->m_Metadata->layer, visited);
        }
        
        m_Layers.push_back(layer);
        m_LayerNodes.push_back(layer->m_InboundNodes.back().get()); // some layers like sequence can 'evolve' in time when new layers are added. we want to remember current configuration
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::TrainStep(const const_tensor_ptr_vec_t& inputs, const const_tensor_ptr_vec_t& outputs, float* loss, float* acc)
    {
        /*assert(ModelInputLayers().size() == inputs.size());
        for (auto i = 0; i < inputs.size(); ++i)
            assert(ModelInputLayers()[i]->InputShape().EqualsIgnoreBatch(inputs[i]->GetShape()));
        assert(ModelOutputLayers().size() == outputs.size());
        for (auto i = 0; i < outputs.size(); ++i)
            assert(ModelOutputLayers()[i]->OutputShape().EqualsIgnoreBatch(outputs[i]->GetShape()));*/

        auto results = m_Trainer->Train(inputs, outputs);

        if (loss)
            *loss = (*results[m_Metrics["loss"].second])(0) / (float)outputs.size();

        Debug::Step();
    }

    //////////////////////////////////////////////////////////////////////////
    tuple<float,float> ModelBase::TrainOnBatch(const Tensor& input, const Tensor& output)
    {
        return TrainOnBatch({ &input }, { &output });
    }

    //////////////////////////////////////////////////////////////////////////
    tuple<float, float> ModelBase::TrainOnBatch(const const_tensor_ptr_vec_t& inputs, const const_tensor_ptr_vec_t& outputs)
    {
        float loss, acc;
        TrainStep(inputs, outputs, &loss, &acc);
        return make_tuple(loss, acc);
    }

    //////////////////////////////////////////////////////////////////////////
    const_tensor_ptr_vec_t ModelBase::GenerateBatch(const const_tensor_ptr_vec_t& inputs, const vector<uint32_t>& batchIndices)
    {
        const_tensor_ptr_vec_t result; // result is a vector of tensors (1 per each input) with multiple (batchIndices.size()) batches in each one of them

        for (uint32_t i = 0; i < inputs.size(); ++i)
        {
            uint32_t batchSize = (uint32_t)batchIndices.size();

            auto t = new Tensor(Shape(inputs[i]->Width(), inputs[i]->Height(), inputs[i]->Depth(), batchSize));

            for (uint32_t b = 0; b < batchSize; ++b)
                inputs[i]->CopyBatchTo(batchIndices[b], b, *t);

            result.push_back(t);
        }

        return result;
    }

    //////////////////////////////////////////////////////////////////////////
    string ModelBase::FilePrefix() const
    {
        string lower = ToLower(Name());
        replace_if(lower.begin(), lower.end(), [](unsigned char c) { return c == ' '; }, '_');
        return lower;
    }

    //////////////////////////////////////////////////////////////////////////
    void ModelBase::LogLine(const string& text, bool print)
    {
        if (m_LogFile && m_LogFile->is_open())
        {
            (*m_LogFile) << text << endl;
            m_LogFile->flush();
        }

        if (print)
            cout << text << "\n";
    }
}
